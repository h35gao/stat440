#--- properties of the multivariate normal --------------------------------

# organized code is essential for good programming.
# I usually create separate files for functions and tests.
source("mvn-functions.R")

#--- test random number generation ---------------------------------------------

# (1) check that E X = mu, var(X) = V

d <- 5
mu <- rnorm(d)
V <- crossprod(matrix(rnorm(d^2), d, d)) # guaranteed +ve def matrix

n <- 1e6
X <- rmvn(n = n, mu = mu, V = V)

# means
rbind(true = mu, sample = colMeans(X))

# maximum absolute and relative difference
max.diff <- function(x1, x2, debug = FALSE) {
  if(debug) browser()
  abs.diff <- abs(x1 - x2) # absolute error
  rel.diff <- abs.diff/abs(x1) # relative error
  c(abs = max(abs.diff), rel = max(rel.diff))
}

max.diff(x1 = mu, x2 = colMeans(X))

# variances
# these errors are typically a bit higher, especially relative error,
# for two reasons:
# (1) the variance of sum(x^k) gets larger and larger as k increases
# (2) some of the true variance components generated by this process
#     can be close to zero
max.diff(x1 = V, x2 = var(X))

# (2) check graphically that
# X ~ N(mu, V)  =>   y = a'X + b ~ N(a'mu + b, a'Va)

a <- rnorm(d)
b <- rnorm(1)
y <- c(X %*% a) + b # iid draws
muy <- sum(a*mu) + b # E[y]
sdy <- sqrt(t(a) %*% V %*% a) # sd(y)
hist(y, breaks = 100, freq = FALSE) # histogram of simulated values
curve(dnorm(x, mean = muy, sd = sdy), add = TRUE, col = "red")

#--- check log-likelihood simplifications ---------------------------------

# generate small dataset
d <- 4
mu <- rnorm(d)
V <- crossprod(matrix(rnorm(d^2), d, d))
n <- 20
X <- rmvn(n = n, mu = mu, V = V)

# generate several values of mu and V
nreps <- 10

Mu <- replicate(nreps, expr = {
  rnorm(d)
})

VV <- replicate(nreps, expr = {
  crossprod(matrix(rnorm(d^2), d, d))
})

# evaluate log-likelihood using log(pdf)

ll1 <- rep(NA, nreps)
for(ii in 1:nreps) {
  # for each mu and V
  ll1[ii] <- 0
  for(jj in 1:n) {
    # log-likelihood sums over n observations
    ll1[ii] <- ll1[ii] + dmvn(x = X[jj,],
                              mu = Mu[,ii], V = VV[,,ii], log = TRUE)
  }
}


# evaluate using simplified log-likelihood
Xbar <- colMeans(X)
S <- var(X) * (n-1) # sample variance divides by n-1

ll2 <- rep(NA, nreps)
for(ii in 1:nreps) {
  ll2[ii] <- lmvn(mu = Mu[,ii], V = VV[,,ii], Xbar = Xbar, S = S, n = n)
}

# check that the two are off by a constant
ll1-ll2
range(diff(ll1-ll2))

#--- optimized loglikelihood ---------------------------------------------------

# Original version invokes "solve" twice and "determinant" once.
# Each of these calculations is O(d) for d-dimensional normals.
# Moreover, "solve" does not know that V is positive-definite.
# In fact, faster and more stable "solve" for +ve-def matrices uses
# cholesky decomposition, which also gives determinant for free.

# pass in sufficient statistics Xbar and S (and n)
# to avoid recalculating for each mu and V
lmvn.fast <- function(mu, V, Xbar, S, n) {
  d <- nrow(S) # dimension of normal
  # calculate V^{-1} S and V^{-1}(Xbar - mu) at the same time
  z <- Xbar - mu
  slv <- solveV(V = V, x = cbind(z, S), ldV = TRUE)
  VS <- slv$y[,1+1:d,drop=FALSE] # V^{-1} S (drop=FALSE keeps output as matrix)
  # (Xbar-mu)'V^{-1}(Xbar-mu)
  xVx <- crossprod(z, slv$y[,1])
  ldV <- slv$ldV # log(|V|)
  -.5 * (sum(diag(VS)) + n * xVx + n * ldV)
}

# check code
ll3 <- rep(NA, nreps)
for(ii in 1:nreps) {
  ll3[ii] <- lmvn.fast(mu = Mu[,ii], V = VV[,,ii], Xbar = Xbar, S = S, n = n)
}

range(diff(ll1-ll3))

# compare timings

# generate data
d <- 100
mu <- rnorm(d)
V <- crossprod(matrix(rnorm(d^2), d, d))
n <- 1000
X <- rmvn(n = n, mu = mu, V = V)
# sufficient statistics
Xbar <- colMeans(X)
S <- var(X) * (n-1)

# calculate loglikelihood over and over
nreps <- 1000
system.time({
  replicate(nreps, expr = {
    lmvn(mu = mu, V = V, Xbar = Xbar, S = S, n = n)
  })
})
system.time({
  replicate(nreps, expr = {
    lmvn.fast(mu = mu, V = V, Xbar = Xbar, S = S, n = n)
  })
})

#--- check the analytic calculations for the MLE ---------------------------

source("mle-check.R") # contains a function for checking the mle

# this function requires that the loglikelihood be coded as a function
# taking a single vector value.
# so let's make two functions for converting (mu, V) <-> theta
# in particular, half of V's elements are redundant so need to take this
# into account.
muV2theta <- function(mu, V) {
  c(mu, V[lower.tri(V, diag = TRUE)])
}
theta2muV <- function(theta) {
  # if d = length(mu), then length(theta) = d + d + (d^2-d)/2
  n <- length(theta)
  d <- (-3 + sqrt(9 + 8*n))/2
  mu <- theta[1:d]
  V <- matrix(NA, d, d)
  V[lower.tri(V, diag = TRUE)] <- theta[(d+1):n]
  V <- t(V)
  V[lower.tri(V, diag = TRUE)] <- theta[(d+1):n]
  list(mu = mu, V = V)
}

# loglikelihood as a function of a single vector
loglik <- function(theta) {
  muV <- theta2muV(theta)
  # check if variance is +ve definite
  if(any(eigen(muV$V)$val <= 0)) return(-Inf)
  lmvn(mu = muV$mu, V = muV$V, Xbar = Xbar, S = S, n = n)
}

# generate small dataset
d <- 4
mu <- rnorm(d)
V <- crossprod(matrix(rnorm(d^2), d, d))
n <- 20
X <- rmvn(n = n, mu = mu, V = V)

# mle
Xbar <- colMeans(X)
S <- var(X) * (n-1)
mu.mle <- Xbar
V.mle <- S/n

# parameter names
# here's a trick for plotting greek symbols
mu.names <- paste0("mu[", 1:d, "]")
V.names <- outer(1:d, 1:d, function(x, y) paste0(x,y))
V.names <- matrix(paste0("V[", V.names, "]"), d, d)
theta.names <- muV2theta(mu = mu.names, V = V.names)
# returns an expression, which plot function interprets as greek symbols
# see ?plotmath
theta.names <- parse(text = theta.names)

mle.check(loglik = loglik,
          theta.mle = muV2theta(mu.mle, V.mle),
          theta.names = theta.names)
